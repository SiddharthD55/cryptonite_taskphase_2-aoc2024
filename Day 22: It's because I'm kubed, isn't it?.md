
## Introduction to Kubernetes

In the past, many organizations used monolithic architecture when building applications. This style involved having a single, unified application, often in the form of one codebase and a single executable deployed as a component. While monolithic applications worked well for certain companies, others encountered challenges when it came to scaling. A significant issue with monolithic applications is that if one part of the system needs more resources, the entire application must be scaled up, even if other parts of the system don't require additional capacity. 

To solve this problem, microservices architecture was introduced. Rather than one large application, microservices break down the system into smaller, independently scalable components. For example, in a streaming application like Netflix, the service handling user registration and billing doesn’t need to scale in the same way as the service managing video streaming. This ability to scale specific services independently made microservices a perfect fit for organizations experiencing fluctuating demand. 

With the adoption of microservices came the need for containers—lightweight units that could package applications and run them anywhere. But with hundreds, or even thousands, of containers running, a tool was needed to manage them effectively. Enter Kubernetes, a powerful container orchestration system.

## What is Kubernetes?

Kubernetes is a system designed to manage, scale, and orchestrate containers within a cluster. Imagine a situation where one of the microservices, running inside a container, is struggling to handle increased traffic. Kubernetes can automatically spin up new containers to distribute the load and balance the traffic between them. This ensures the service remains responsive and highly available. 

Kubernetes' ability to automate these processes and scale microservices seamlessly has made it a go-to solution for modern cloud-native applications. It works by grouping containers into pods, which run on nodes, and a cluster is a collection of these nodes. The flexibility and scalability offered by Kubernetes is one of the reasons it has become so popular, providing solutions across different tech stacks and ensuring applications remain available, scalable, and efficient.

## Digital Forensics and Incident Response (DFIR)

In the world of cybersecurity, DFIR stands for Digital Forensics and Incident Response. When a security incident occurs, DFIR professionals are called in to investigate, contain, and recover from the attack. Their job can be split into two main areas:

### Digital Forensics:
Digital forensics is about collecting and analyzing digital evidence to understand what happened during the attack. This process involves identifying, preserving, and analyzing data from affected systems, often after the incident has already occurred. Forensics experts trace the steps of the attacker, helping to reconstruct the timeline of the event and uncover how the systems were compromised.

### Incident Response:
Incident response, on the other hand, focuses on the immediate actions required to contain the damage. This could include isolating infected systems, identifying the vulnerability that was exploited, and recovering systems to a secure state. Incident responders essentially aim to stop the attack from spreading and limit its impact on the organization.

Both roles require strong documentation, as findings are shared to improve security practices and inform future responses. The forensic analyst presents findings as evidence to help identify the attacker and understand their methods, while the incident responder implements strategies to mitigate similar incidents in the future.

## DFIR in Kubernetes: The Ephemeral Challenge

One of the most significant challenges in a Kubernetes environment is the ephemeral nature of containers. Containers often have short lifespans—sometimes only lasting minutes—especially when they are spun up to handle increased load or perform specific jobs. In fact, according to the 2024 Cloud-Native Security and Usage Report by Sysdig, 70% of containers live for less than five minutes.

This transience presents a problem for DFIR teams, as logs and evidence generated by a compromised container may disappear before they can be captured. So how do we investigate such incidents in Kubernetes environments?

### Improving Visibility with Kubernetes Audit Logging

The key to dealing with ephemeral workloads in Kubernetes is increasing visibility. Kubernetes provides audit logging, which captures API requests and events at various stages, even after the resources they relate to have been deleted. For example, if a user initiates a request to delete a pod, the pod may be deleted, but the request itself is logged in the audit logs. By using these logs, DFIR professionals can track what happened, when it happened, and who initiated it.

Audit logs capture detailed information, including:

- What happened (e.g., deletion of a pod)
- When it happened
- Who initiated the action
- To what resource it happened (e.g., a specific pod)
- Where it was observed (e.g., a particular node)
- From where the request originated (e.g., an IP address)

These audit logs are invaluable in understanding the scope of an incident and reconstructing the timeline of events.

### Building a Comprehensive DFIR Toolkit

To effectively investigate and respond to security incidents in a Kubernetes environment, we need to supplement audit logs with other runtime security tools. These tools can aggregate and analyze events from multiple sources, transforming raw data into actionable insights. This allows security teams to visualize events, spot suspicious patterns, and respond to incidents faster. Investing in a well-rounded toolkit is essential for any DFIR professional working with cloud-native applications.

Let's begin our investigation. As previously mentioned, some of the log sources can disappear due to the ephemeral nature of pod-based services. We'll see this in action now. On the VM, open a terminal and start Kubernetes using the following command:

```bash
ubuntu@tryhackme:~$ minikube start
minikube v1.32.0 on Ubuntu 20.04
Using the docker driver based on existing profile
Starting control plane node minikube in cluster minikube

--- removed for brevity ---

Enabled addons: storage-provisioner, default-storageclass
Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
It will take roughly three minutes for the cluster to configure itself and start. I can verify that the cluster is up and running using the following command:

ubuntu@tryhackme:~$ kubectl get pods -n wareville
NAME                              READY   STATUS    RESTARTS         AGE
morality-checker                  1/1     Running   8  (9m16s ago)   20d
naughty-or-nice                   1/1     Running   1  (9m16s ago)    9d
naughty-picker-7cbd95dd66-gjm7r   1/1     Running   32 (9m16s ago)   20d
naughty-picker-7cbd95dd66-gshvp   1/1     Running   32 (9m16s ago)   20d
nice-picker-7cd98989c8-bfbqn      1/1     Running   32 (9m16s ago)   20d
nice-picker-7cd98989c8-ttc7t      1/1     Running   32 (9m16s ago)   20d
```

If all of the pods are up and running (based on their status), I am ready to proceed. Since we know the web application was compromised, let's connect to that pod and see if we can recover any logs. Connect to the pod using the following command:

```bash
ubuntu@tryhackme:~$ kubectl exec -n wareville naughty-or-nice -it -- /bin/bash
root@naughty-or-nice:/#
```

Once connected, review the Apache2 access log:

```bash
root@naughty-or-nice:/# cat /var/log/apache2/access.log
172.17.0.1 - - [28/Oct/2024:11:05:45 +0000] "GET / HTTP/1.1" 200 2038 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:131.0) Gecko/20100101 Firefox/131.0"
172.17.0.1 - - [28/Oct/2024:11:05:45 +0000] "GET /style/style.css HTTP/1.1" 200 1207 "http://localhost:8081/" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:131.0) Gecko/20100101 Firefox/131.0"

--- removed for brevity ---

172.17.0.1 - - [29/Oct/2024:12:32:37 +0000] "GET /favicon.ico HTTP/1.1" 404 489 "http://localhost:8081/" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/113.0"
172.17.0.1 - - [29/Oct/2024:12:32:48 +0000] "GET /shelly.php?cmd=whoami HTTP/1.1" 200 224 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/113.0"
```

Sadly, we only see logs from October 28th, when the attack occurred later. However, looking at the last log, we notice an interesting request to a `shelly.php` file. This indicates we are on the right track. We'll terminate our session to the pod using `exit`. Fortunately, McSkidy knew that the log source was ephemeral and ensured remote backups of the log source were made. Navigate to the backup directory:

```bash
cd /home/ubuntu/dfir_artefacts/
```

Here, we will find the access logs stored in `pod_apache2_access.log`. Review these logs to see what Mayor Malware was doing on the website and answer the first three questions at the bottom of the task!

At this point, our investigation faces a challenge. Since the pod was configured using a port-forward, we can't see the actual IP that connected to the instance. Additionally, we still don't fully understand how the webshell made its way into the pod. However, after rebooting the cluster, we found the webshell present, which suggests it is embedded within the actual image of the pod. This means we need to investigate the Docker image registry itself.

To view the registry container ID, run the following command:

```bash
ubuntu@tryhackme:~$ docker ps
CONTAINER ID   IMAGE         COMMAND                  --- removed for brevity ---
77fddf1ff1b8   registry:2.7 "/entrypoint.sh /etc…"    --- removed for brevity ---
cd9ee77b8aa5   gcr.io/k8s-minikube/kicbase:v0.0.42    --- removed for brevity ---
```

Now, let's connect to the instance to see if we have any logs:

```bash
ubuntu@tryhackme:~$ docker exec CONTAINER_NAME_OR_ID ls -al /var/log
total 12
drwxr-xr-x    2 root     root          4096 Nov 12  2021 .
drwxr-xr-x    1 root     root          4096 Nov 12  2021 ..
```

Again, we hit a wall as there are no registry logs. Fortunately, Docker itself keeps logs. Let's pull these logs using the following command:

```bash
ubuntu@tryhackme:~$ docker logs CONTAINER_NAME_OR_ID
172.17.0.1 - - [16/Oct/2024:09:02:39 +0000] "GET /v2/ HTTP/1.1" 401 87 "" "docker/26.0.0 go/go1.21.8 git-commit/8b79278 kernel/5.15.0-1070-aws os/linux arch/amd64 UpstreamClient(Docker-Client/26.0.0 \\(linux\\))"
172.17.0.1 - - [16/Oct/2024:09:02:39 +0000] "GET /v2/ HTTP/1.1" 401 87 "" "docker/26.0.0 go/go1.21.8 git-commit/8b79278 kernel/5.15.0-1070-aws os/linux arch/amd64 UpstreamClient(Docker-Client/26.0.0 \\(linux\\))"

--- removed for brevity ---

time="2024-11-08T04:32:42.87960937Z" level=info msg="using inmemory blob descriptor cache" go.version=go1.11.2 instance.id=ef35cf6e-fd01-4041-abba-2c082fd682f0 service=registry version=v2.7.1 
time="2024-11-08T04:32:42.880803524Z" level=info msg="listening on [::]:5000" go.version=go1.11.2 instance.id=ef35cf6e-fd01-4041-abba-2c082fd682f0 service=registry version=v2.7.1
```

Now we have something useful! These logs have been saved for us in the `/home/ubuntu/dfir_artefacts/docker-registry-logs.log` file. Let’s begin by checking all the different connections to the registry by searching for the `HEAD` HTTP request code and restricting the output to just the IPs:

```bash
ubuntu@tryhackme:~/dfir_artefacts$ cat docker-registry-logs.log | grep "HEAD" | cut -d ' ' -f 1
172.17.0.1
172.17.0.1
172.17.0.1

--- removed for brevity ---

10.10.130.253
10.10.130.253
10.10.130.253
```

Here we observe that most of the connections to our registry were made from the expected IP (`172.17.0.1`), but we also see connections from `10.10.130.253`, an unexpected IP. Let's further investigate all requests made by this IP:

```bash
ubuntu@tryhackme:~/dfir_artefacts$ cat docker-registry-logs.log | grep "10.10.130.253"
10.10.130.253 - - [29/Oct/2024:10:06:33 +0000] "GET /v2/ HTTP/1.1" 401 87 "" "docker/19.03.12 go/go1.13.10 git-commit/48a66213fe kernel/4.15.0-213-generic os/linux arch/amd64 UpstreamClient(Docker-Client/19.03.12 \\(linux\\))"
10.10.130.253 - - [29/Oct/2024:10:06:33 +0000] "GET /v2/ HTTP/1.1" 200 2 "" "docker/19.03.12 go/go1.13.10 git-commit/48a66213fe kernel/4.15.0-213-generic os/linux arch/amd64 UpstreamClient(Docker-Client/19.03.12 \\(linux\\))"

--- removed for brevity ---

10.10.130.253 - - [29/Oct/2024:12:34:31 +0000] "PUT /v2/wishlistweb/manifests/latest HTTP/1.1" 201 0 "" "docker/19.03.12 go/go1.13.10 git-commit/48a66213fe kernel/4.15.0-213-generic os/linux arch/amd64 UpstreamClient(Docker-Client/19.03.12 \\(linux\\))"
```

By reviewing the first few requests, we see several authentication attempts. We also observe that the request to read the manifest for the `wishlistweb` image succeeded, indicated by the HTTP status code `200`:

```bash
10.10.130.253 - - [29/Oct/2024:12:26:40 +0000] "GET /v2/wishlistweb/manifests/latest HTTP/1.1" 200 6366 "" "docker/19.03.12 go/go1.13.10 git-commit/48a66213fe kernel/4.15.0-213-generic os/linux arch/amd64 UpstreamClient(Docker-Client/19.03.12 \\(linux\\))"
```

The `docker` user agent confirms that these requests were made using the Docker CLI. We also see several requests for downloading the image. From this, we deduce several key points:

- The Docker CLI application was used to connect to the registry.
- Connections came from `10.10.130.253`, an unexpected IP, as we typically upload images from `172.17.0.1`.
- The client was authenticated, allowing them to pull the image. This means the attacker had access to credentials.

If they had credentials to pull an image, they could also push a new image. We can verify this by searching for any `PATCH` HTTP methods, as `PATCH` is used to update Docker images in a registry:

```bash
ubuntu@tryhackme:~/dfir_artefacts$ cat docker-registry-logs.log | grep "10.10.130.253" | grep "PATCH"
10.10.130.253 - - [29/Oct/2024:12:34:28 +0000] "PATCH /v2/wishlistweb/blobs/uploads/2966 --- removed for brevity ---
10.10.130.253 - - [29/Oct/2024:12:34:31 +0000] "PATCH /v2/wishlistweb/blobs/uploads/7d53 --- removed for brevity ---
```

From these logs, we confirm that the attacker used the `PATCH` method to upload data to the `wishlistweb` image, confirming that they had the ability to modify the image.

**The Attack Path:**

1. **Rolebinding and Permissions:**
   Mayor Malware was assigned the role `mayor-user` within the Kubernetes environment. The role was bound to the user through the `mayor-user-binding`. Upon inspecting the role `mayor-user`, it became clear that it had certain permissions:
   - **Pods/exec:** This permission allowed Mayor Malware to execute commands inside containers. This is a critical permission, as it could be used to inject a web shell or perform other malicious activities inside the pods.
   - **Rolebindings and Roles:** Mayor Malware was granted the ability to describe roles and role bindings. These permissions gave him visibility into which roles existed in the Kubernetes cluster and the associated permissions.

   The critical permission here was **pods/exec**, as it allowed Mayor Malware to interact with containers, which later enabled him to insert a web shell into the container.

2. **Attempt to List Secrets:**
   The logs showed that Mayor Malware tried to list secrets in the `wareville` namespace. However, his attempt was denied with a **403 Forbidden** error, as the role assigned to him didn't have permission to access secrets directly. This event is logged with a response status of **Failure** and a message stating: `secrets is forbidden: User "mayor-malware" cannot list resource "secrets"`. This shows that despite having access to some Kubernetes resources, Mayor Malware was not authorized to access sensitive secrets directly.

3. **Accessing Roles and Finding a Vulnerability:**
   After being denied access to secrets, Mayor Malware turned his attention to the roles within the `wareville` namespace. Through the `kubectl get roles` and `kubectl describe roles` commands, he found a role called **job-runner** that had the permission to read secrets. This was a key turning point in the attack, as Mayor Malware now had insight into which role had access to secrets.

   The logs show that Mayor Malware successfully queried the role `job-runner` and learned that this role could read secrets. Although his own role (`mayor-user`) didn't grant him the ability to read secrets, the discovery of the `job-runner` role, which had such access, provided him the opportunity to exploit it.

4. **Exploiting the Role with `pods/exec`:**
   Equipped with the knowledge of the `job-runner` role's access to secrets, Mayor Malware likely proceeded to escalate his privileges by either exploiting a misconfigured role or gaining access to a pod that was running with the `job-runner` role's permissions. Since his own role allowed him to execute commands within pods (`pods/exec`), this would have enabled him to execute code inside containers that were running with the `job-runner` role or related permissions. This could have included executing a web shell that facilitated further malicious actions within the environment.

5. **Compromise of the Docker Registry Image:**
   Finally, through access to a pod (likely through the `pods/exec` permission), Mayor Malware could have injected a web shell into a running container. This compromised container, when pushed back to the Docker registry, would allow Mayor Malware to push a malicious image update, which explained how the webshell made its way into the image. This backdoor would then be available for future exploitation.

**Conclusion and Recommendations:**

- The key issue that facilitated this attack was the permission associated with `pods/exec`, which allowed Mayor Malware to execute commands inside containers. This could have been better restricted to only those users who truly needed it.
- The discovery of the `job-runner` role's permissions to read secrets was another critical element. Even though Mayor Malware didn’t have direct access to secrets, he was able to gather enough information to exploit the Kubernetes environment.
- To prevent such attacks in the future, **role-based access controls (RBAC)** should be reviewed and refined to ensure that sensitive permissions (like `pods/exec` and secret access) are not granted unnecessarily. Specifically, roles that grant broad access should be split into more granular roles, reducing the risk of privilege escalation.
- **Audit logs** and **monitoring** should be configured to detect suspicious activities such as unauthorized role access, attempts to describe roles with sensitive permissions, or execution inside containers by non-administrative users.

In this case, Mayor Malware's access to the Kubernetes environment through overly permissive roles, combined with the ability to execute commands in containers, allowed him to compromise the Docker registry image. Understanding this attack path will help strengthen defenses and minimize the risk of similar incidents.

### Sequence of Events

#### 1. **Rolebinding Discovery**

Mayor Malware initially discovers a role binding that gives a service account permission to view secrets. The role binding in question, `mayor-user-binding/wareville`, grants the user `mayor-malware` access to the `mayor-user` role. The relevant audit log entry shows the following:

```
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"25b7417e-550c-4b9a-bb2c-dad64662cce0","stage":"ResponseComplete","requestURI":"/apis/rbac.authorization.k8s.io/v1/namespaces/wareville/rolebindings?limit=500","verb":"list","user":{"username":"mayor-malware","groups":["example","system:authenticated"]},"sourceIPs":["192.168.49.1"],"userAgent":"kubectl/v1.29.3 (linux/amd64) kubernetes/6813625","objectRef":{"resource":"rolebindings","namespace":"wareville","apiGroup":"rbac.authorization.k8s.io","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestReceivedTimestamp":"2024-10-29T12:20:59.570824Z","stageTimestamp":"2024-10-29T12:20:59.575620Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by RoleBinding \"mayor-user-binding/wareville\" of Role \"mayor-user\" to User \"mayor-malware\""}}
```

#### 2. **Role Binding Description**

To further investigate, Mayor Malware describes the `job-runner-binding` role binding and discovers that this role is linked to the service account `job-runner-sa`. This service account possesses the permissions necessary to view secrets. This step reveals that a legitimate path exists to access sensitive data.

```
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"b0f9aa98-9039-4df8-b990-9bf6ca48ab2f","stage":"ResponseComplete","requestURI":"/apis/rbac.authorization.k8s.io/v1/namespaces/wareville/rolebindings/job-runner-binding","verb":"get","user":{"username":"mayor-malware","groups":["example","system:authenticated"]},"sourceIPs":["192.168.49.1"],"userAgent":"kubectl/v1.29.3 (linux/amd64) kubernetes/6813625","objectRef":{"resource":"rolebindings","namespace":"wareville","name":"job-runner-binding","apiGroup":"rbac.authorization.k8s.io","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestReceivedTimestamp":"2024-10-29T12:21:11.521236Z","stageTimestamp":"2024-10-29T12:21:11.523301Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by RoleBinding \"mayor-user-binding/wareville\" of Role \"mayor-user\" to User \"mayor-malware\""}}
```

#### 3. **Pod Listing and Description**

Armed with this knowledge, Mayor Malware proceeds to list all the pods in the `wareville` namespace and then describes the pod `morality-checker`, which is running with the `job-runner-sa` service account attached. This confirms that if he could gain access to the pod, he would be able to access the secrets associated with the service account.

```
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"5965471b-4fb9-49c9-9a16-7fd466c762c8","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/wareville/pods/morality-checker","verb":"get","user":{"username":"mayor-malware","groups":["example","system:authenticated"]},"sourceIPs":["192.168.49.1"],"userAgent":"kubectl/v1.29.3 (linux/amd64) kubernetes/6813625","objectRef":{"resource":"pods","namespace":"wareville","name":"morality-checker","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestReceivedTimestamp":"2024-10-29T12:21:33.182365Z","stageTimestamp":"2024-10-29T12:21:33.185006Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by RoleBinding \"mayor-user-binding/wareville\" of Role \"mayor-user\" to User \"mayor-malware\""}}
```

#### 4. **Exploit Execution: `exec` Command**

With the knowledge that the service account attached to the `morality-checker` pod has access to secrets, Mayor Malware runs an `exec` command within the pod. This action is logged as follows:

```
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"927fcde7-74e5-4a57-af53-dceacefaf47c","stage":"ResponseStarted","requestURI":"/api/v1/namespaces/wareville/pods/morality-checker/exec?command=%2Fbin%2Fsh\u0026container=kubectl-container\u0026stdin=true\u0026stdout=true\u0026tty=true","verb":"create","user":{"username":"mayor-malware","groups":["example","system:authenticated"]},"sourceIPs":["192.168.49.1"],"userAgent":"kubectl/v1.29.3 (linux/amd64) kubernetes/6813625","objectRef":{"resource":"pods","namespace":"wareville","name":"morality-checker","apiVersion":"v1","subresource":"exec"},"responseStatus":{"metadata":{},"code":101},"requestReceivedTimestamp":"2024-10-29T12:21:44.189258Z","stageTimestamp":"2024-10-29T12:21:44.214173Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by RoleBinding \"mayor-user-binding/wareville\" of Role \"mayor-user\" to User \"mayor-malware\""}}
```

This confirms that RBAC misconfigurations in the Kubernetes environment allowed Mayor Malware to escalate his privileges and execute a shell inside the `morality-checker` pod.

#### 5. **Accessing Secrets: `job-runner-sa` Service Account Logs**

Next, McSkidy, the security analyst, examines the logs for the `job-runner-sa` service account. This service account runs commands to access secrets, including listing and retrieving the `pull-creds` secret, which contains sensitive docker credentials.

```
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"RequestResponse","auditID":"c59d6a7c-1e07-43cb-8bf6-4d41a9c98ddb","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/wareville/secrets/pull-creds","verb":"get","user":{"username":"system:serviceaccount:wareville:job-runner-sa","uid":"9e88bb94-e5e3-4e13-9187-4eaf898d0a7e","groups":["system:serviceaccounts","system:serviceaccounts:wareville","system:authenticated"],"extra":{"authentication.kubernetes.io/pod-name":["morality-checker"],"authentication.kubernetes.io/pod-uid":["a20761b8-1a36-4318-a048-96d61644b436"]}},"sourceIPs":["10.244.120.126"],"userAgent":"kubectl/v1.31.1 (linux/amd64) kubernetes/948afe5","objectRef":{"resource":"secrets","namespace":"wareville","name":"pull-creds","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"responseObject":{"kind":"Secret","apiVersion":"v1","metadata":{"name":"pull-creds","namespace":"wareville","uid":"c3854acc-f67b-4e82-a975-816e0c6ab04b","resourceVersion":"174795","creationTimestamp":"2024-10-17T18:10:27Z","managedFields":[{"manager":"kubectl-create","operation":"Update","apiVersion":"v1","time":"2024-10-17T18:10:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:data":{".":{},"f:.dockerconfigjson":{}},"f:type":{}}}]},"data":{".dockerconfigjson":"eyJhdXRocyI6eyJodHRwOi8vZG9ja2VyLXJlZ2lzdHJ5Lm5pY2V0b3duLmxvYzo1MDAwIjp7InVzZXJuYW1lIjoibXIubmljZSIsInBhc3N3b3JkIjoiTXIuTjR1Z2h0eSIsImF1dGgiOiJiWEl1Ym1qalpUcE5jaTVPTkhWbmFIUjUifX19"},"type":"kubernetes.io/dockerconfigjson"},"requestReceivedTimestamp":"2024-10-29T12:22:15.861424Z","stageTimestamp":"2024-10-29T12:22:15.864166Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by RoleBinding \"job-runner-binding/wareville\" of Role \"job-runner\" to ServiceAccount \"job-runner-sa/wareville\""}}
```

#### 6. **Final Confirmation of the Attack Path**

To confirm the final step of the attack, McSkidy retrieves and decodes the docker configuration stored in the `pull-creds` secret. The following command is used to decode the base64-encoded data:

```
kubectl get secret pull-creds -n wareville -o jsonpath='{.data.\.dockerconfigjson}' | base64 --decode
```

The credentials obtained from the `pull-creds` secret show that the same password is used for both pulling and pushing images to the docker registry. With this, Mayor Malware gained access to the registry, pushing a malicious web shell into the Kubernetes environment and ensuring persistence.

**Answers**

*What is the name of the webshell that was used by Mayor Malware?* **shelly.php**

*What file did Mayor Malware read from the pod?* **db.php**

*What tool did Mayor Malware search for that could be used to create a remote connection from the pod?* **nc**

*What IP connected to the docker registry that was unexpected?* **10.10.130.253**

*At what time is the first connection made from this IP to the docker registry?* **29/Oct/2024:10:06:33 +0000**

*At what time is the updated malicious image pushed to the registry?* **29/Oct/2024:12:34:28 +0000**

*What is the value stored in the "pull-creds" secret?* **{"auths":{"http://docker-registry.nicetown.loc:5000":{"username":"mr.nice","password":"Mr.N4ughty","auth":"bXIubmljZTpNci5ONHVnaHR5"}}}**
